{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PkmFBsVtoZC"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from os import listdir\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image,  ImageOps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuU02Z24tsLF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_n02YhaUuoj"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "num_channels = 1\n",
        "num_features = 8\n",
        "image_size = 128\n",
        "latent_dim = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cn-zW7Uq7UbU"
      },
      "outputs": [],
      "source": [
        "result_path = '/content/drive/MyDrive/Hossein/CancerCell/result/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAS-PwLlGLNZ"
      },
      "outputs": [],
      "source": [
        "weight_path = '/content/drive/MyDrive/Hossein/CancerCell/weight/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_path = '/content/drive/MyDrive/Hossein/CancerCell/logs'"
      ],
      "metadata": {
        "id": "rGT5liuqm2ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0BuDgLitoZK"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Hossein/CancerCell/MyExpt_IdentifySecondaryObjects.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbDjuq9QX5Ou"
      },
      "outputs": [],
      "source": [
        "df['Id'] = df['Metadata_Well']+'_'+df['ImageNumber'].astype('str')+'_'+df['ObjectNumber'].astype('str')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vonCTcMNtoZP"
      },
      "outputs": [],
      "source": [
        "data = df[['Id','AreaShape_Area', 'AreaShape_BoundingBoxMinimum_Y',\n",
        "'AreaShape_Eccentricity', 'AreaShape_HuMoment_4',\n",
        "'AreaShape_NormalizedMoment_1_1', 'AreaShape_NormalizedMoment_1_2',\n",
        "'AreaShape_Zernike_6_2', 'AreaShape_Zernike_8_6',\n",
        "'Intensity_MaxIntensity_DNA']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_XNFIwttoZT"
      },
      "outputs": [],
      "source": [
        "img_path = '/content/drive/MyDrive/Ravaee/CancerCell/processed_June22/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL7I54z2toZU"
      },
      "outputs": [],
      "source": [
        "img_files = listdir(img_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuQfrNmlu_Zu"
      },
      "outputs": [],
      "source": [
        "len(img_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBQR3AY1toZc"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aad1NjfptoZg"
      },
      "outputs": [],
      "source": [
        "scaler = scaler.fit(data[['AreaShape_Area', 'AreaShape_BoundingBoxMinimum_Y',\n",
        "'AreaShape_Eccentricity', 'AreaShape_HuMoment_4',\n",
        "'AreaShape_NormalizedMoment_1_1', 'AreaShape_NormalizedMoment_1_2',\n",
        "'AreaShape_Zernike_6_2', 'AreaShape_Zernike_8_6',\n",
        "'Intensity_MaxIntensity_DNA']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkdy4XUCbOrH"
      },
      "outputs": [],
      "source": [
        "Id = data['Id'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0wrPJAwtoZh"
      },
      "outputs": [],
      "source": [
        "data=scaler.transform(data[['AreaShape_Area', 'AreaShape_BoundingBoxMinimum_Y',\n",
        "'AreaShape_Eccentricity', 'AreaShape_HuMoment_4',\n",
        "'AreaShape_NormalizedMoment_1_1', 'AreaShape_NormalizedMoment_1_2',\n",
        "'AreaShape_Zernike_6_2', 'AreaShape_Zernike_8_6',\n",
        "'Intensity_MaxIntensity_DNA']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edkRXZxgtzOG"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({'id':Id,'AreaShape_Area':data[:,0],\n",
        "                                'AreaShape_BoundingBoxMinimum_Y':data[:,1],\n",
        "                               'AreaShape_Eccentricity':data[:,2],\n",
        "                                'AreaShape_HuMoment_4':data[:,3],\n",
        "                                 'AreaShape_NormalizedMoment_1_1':data[:,4],\n",
        "                                'AreaShape_NormalizedMoment_1_2':data[:,5],\n",
        "                                'AreaShape_Zernike_6_2':data[:,6],\n",
        "                                'AreaShape_Zernike_8_6':data[:,7],\n",
        "                                 'Intensity_MaxIntensity_DNA':data[:,8]\n",
        "                   })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsgGBch_toZj"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((df['id'].values,\n",
        "                                              tf.cast(df['AreaShape_Area'].values, tf.float32),\n",
        "                                              tf.cast(df['AreaShape_Eccentricity'].values, tf.float32),\n",
        "                                              tf.cast(df['AreaShape_BoundingBoxMinimum_Y'].values, tf.float32),\n",
        "                                              tf.cast(df['AreaShape_HuMoment_4'].values, tf.float32),\n",
        "                                             tf.cast(df['AreaShape_NormalizedMoment_1_1'].values, tf.float32),\n",
        "                                             tf.cast(df['AreaShape_NormalizedMoment_1_2'].values, tf.float32),\n",
        "                                             tf.cast(df['AreaShape_Zernike_6_2'].values, tf.float32),\n",
        "                                             tf.cast(df['AreaShape_Zernike_8_6'].values, tf.float32),\n",
        "                                             tf.cast(df['Intensity_MaxIntensity_DNA'].values, tf.float32),\n",
        "                                                   ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwgHMdmktoZj"
      },
      "outputs": [],
      "source": [
        "def map_fn(path, l1,l2,l3,l4,l5,l6,l7,l8,l9):\n",
        "    image = tf.image.decode_jpeg(tf.io.read_file(img_path+path+'.jpg'))\n",
        "    image = (tf.cast(image, tf.float32)-127.5) / 127.5\n",
        "    return image, tf.convert_to_tensor([l1,l2,l3,l4,l5,l6,l7,l8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5YOIhBCtoZl"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.map(map_fn)\n",
        "train_dataset = (train_dataset\n",
        "    .shuffle(1024)\n",
        "    .cache()\n",
        "#     .repeat()\n",
        "    .batch(batch_size)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfutsBiNUsXR"
      },
      "outputs": [],
      "source": [
        "def get_discriminator():\n",
        "  input_layer = layers.Input(shape=(64,64,num_channels+1))\n",
        "  x = layers.Conv2D(32 , (7,7) , padding='same',strides=2,use_bias=True)(input_layer)\n",
        "  x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "  x = layers.Conv2D(64 , (7,7) , padding='same',strides=2,use_bias=True)(x)\n",
        "  x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "  x = layers.Conv2D(128 , (7,7) , padding='same',strides=2,use_bias=True)(x)\n",
        "  x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "  x = layers.Conv2D(256 , (7,7) , padding='same',strides=2,use_bias=True)(x)\n",
        "  x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "  x = layers.Flatten()(x)\n",
        "  x = layers.Dropout(0.2)(x)\n",
        "  x = layers.Dense(256 , activation='relu')(x)\n",
        "  x = layers.Dropout(0.2)(x)\n",
        "  x = layers.Dense(1)(x)\n",
        "  model = tf.keras.models.Model(input_layer,x, name='discriminator')\n",
        "  model.summary()\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GX6OuV2Mgm0z"
      },
      "outputs": [],
      "source": [
        "def get_geneator():\n",
        "  input_layer = layers.Input(shape=(latent_dim+num_features))\n",
        "  x = layers.Dense(8 * 8 * (latent_dim+num_features))(input_layer)\n",
        "  # x = layers.BatchNormalization()(x)\n",
        "  x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "  x = layers.Reshape((8, 8, latent_dim+num_features))(x)\n",
        "  x = layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding=\"same\",use_bias=True)(x)\n",
        "  # x = layers.BatchNormalization()(x)\n",
        "  x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "  x = layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding=\"same\",use_bias=True)(x)\n",
        "  # x = layers.BatchNormalization()(x)\n",
        "  x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "  x = layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding=\"same\",use_bias=True)(x)\n",
        "  # x = layers.BatchNormalization()(x)\n",
        "  x = layers.Activation(\"tanh\")(x)\n",
        "  model = tf.keras.models.Model(input_layer,x, name='generator')\n",
        "  model.summary()\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmqT0R1z4jBo"
      },
      "outputs": [],
      "source": [
        "class GANMonitor(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=128 , feature=[]):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "        self.features = np.vstack([feature]*num_img)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        random_vector_features = tf.concat(\n",
        "            [random_latent_vectors, self.features], axis=1\n",
        "        )\n",
        "\n",
        "        # Decode the noise (guided by features) to fake images.\n",
        "        generated_images = self.model.generator(random_vector_features)\n",
        "        generated_images = (generated_images * 127.5) + 127.5\n",
        "\n",
        "        for i in range(self.num_img):\n",
        "            img = generated_images[i].numpy()\n",
        "            img = tf.keras.preprocessing.image.array_to_img(img)\n",
        "            img.save(result_path+\"generated_img_{epoch}_{i}.png\".format(i=i, epoch=epoch))\n",
        "\n",
        "        self.model.generator.save(weight_path+'generator.h5')\n",
        "        self.model.discriminator.save(weight_path+'discriminator.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdL0vxLdtoZr"
      },
      "outputs": [],
      "source": [
        "class ConditionalGAN(tf.keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim,batch_size):\n",
        "        super(ConditionalGAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.gen_loss_tracker = tf.keras.metrics.Mean(name=\"generator_loss\")\n",
        "        self.disc_loss_tracker = tf.keras.metrics.Mean(name=\"discriminator_loss\")\n",
        "        self.batch_size = batch_size\n",
        "        self.d_steps = 3\n",
        "        self.gp_weight = 10\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
        "        # return [self.gen_loss_tracker, tf.keras.metrics.Accuracy()]\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, d_loss_fn,g_loss_fn):\n",
        "        super(ConditionalGAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.d_loss_fn = d_loss_fn\n",
        "        self.g_loss_fn = g_loss_fn\n",
        "\n",
        "    def gradient_penalty(self, batch_size, real_images, fake_images):\n",
        "        \"\"\" Calculates the gradient penalty.\n",
        "\n",
        "        This loss is calculated on an interpolated image\n",
        "        and added to the discriminator loss.\n",
        "        \"\"\"\n",
        "        # Get the interpolated image\n",
        "        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
        "        diff = fake_images - real_images\n",
        "        interpolated = real_images + alpha * diff\n",
        "\n",
        "        with tf.GradientTape() as gp_tape:\n",
        "            gp_tape.watch(interpolated)\n",
        "            # 1. Get the discriminator output for this interpolated image.\n",
        "            pred = self.discriminator(interpolated, training=True)\n",
        "\n",
        "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
        "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
        "        # 3. Calculate the norm of the gradients.\n",
        "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
        "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
        "        return gp\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack the data.\n",
        "        real_images, features = data\n",
        "\n",
        "        for i in range(self.d_steps):\n",
        "            pad = image_size * image_size // num_features\n",
        "            image_of_features = tf.repeat(\n",
        "                      features, repeats=[pad] ,axis=1\n",
        "                  )\n",
        "            image_of_features = tf.reshape(\n",
        "                image_of_features, (-1, image_size, image_size, 1)\n",
        "            )\n",
        "            batch_size = tf.shape(real_images)[0]\n",
        "            random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "            random_vector_features = tf.concat(\n",
        "                [random_latent_vectors, features], axis=1\n",
        "            )\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                self.discriminator.trainable = True\n",
        "                # Generate fake images from the latent vector\n",
        "                fake_images = self.generator(random_vector_features,training=True)\n",
        "                fake_image_and_features = tf.concat([fake_images, image_of_features], -1)\n",
        "                real_image_and_features = tf.concat([real_images, image_of_features], -1)\n",
        "\n",
        "                # Get the logits for the fake images\n",
        "                fake_logits = self.discriminator(fake_image_and_features, training=True)\n",
        "\n",
        "                # Get the logits for the real images\n",
        "                real_logits = self.discriminator(real_image_and_features, training=True)\n",
        "\n",
        "                # Calculate the discriminator loss using the fake and real image logits\n",
        "                d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
        "                # Calculate the gradient penalty\n",
        "                gp = self.gradient_penalty(batch_size, real_image_and_features, fake_image_and_features)\n",
        "                # Add the gradient penalty to the original discriminator loss\n",
        "                d_loss = d_cost + gp * self.gp_weight\n",
        "\n",
        "            # Get the gradients w.r.t the discriminator loss\n",
        "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
        "            # Update the weights of the discriminator using the discriminator optimizer\n",
        "            self.d_optimizer.apply_gradients(\n",
        "                zip(d_gradient, self.discriminator.trainable_variables)\n",
        "            )\n",
        "\n",
        "\n",
        "        # Train the generator\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        random_vector_features = tf.concat(\n",
        "                [random_latent_vectors, features], axis=1\n",
        "            )\n",
        "\n",
        "        with tf.GradientTape() as g_tape:\n",
        "            self.discriminator.trainable = False\n",
        "\n",
        "            g_tape.watch(self.generator.trainable_variables)\n",
        "\n",
        "            # Generate fake images using the generator\n",
        "            generated_images = self.generator(random_vector_features, training=True)\n",
        "            # Get the discriminator logits for fake images\n",
        "            fake_image_and_features = tf.concat([generated_images, image_of_features], -1)\n",
        "            gen_img_logits = self.discriminator(fake_image_and_features, training=True)\n",
        "\n",
        "            # Calculate the generator loss\n",
        "            g_loss = self.g_loss_fn(gen_img_logits)\n",
        "\n",
        "        # Get the gradients w.r.t the generator loss\n",
        "        gen_gradient = g_tape.gradient(g_loss, self.generator.trainable_variables)\n",
        "        # Update the weights of the generator using the generator optimizer\n",
        "        self.g_optimizer.apply_gradients(\n",
        "            zip(gen_gradient, self.generator.trainable_variables)\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        # Monitor loss.\n",
        "        self.gen_loss_tracker.update_state(g_loss)\n",
        "        self.disc_loss_tracker.update_state(d_loss)\n",
        "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0NwoYmK9fMf"
      },
      "outputs": [],
      "source": [
        "path = df.iloc[0,:]['id']\n",
        "img=Image.open('/content/drive/MyDrive/Hossein/CancerCell/processed_June22/'+path+'.jpg')\n",
        "display(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBmLHZZ59PvG"
      },
      "outputs": [],
      "source": [
        "x1  = df.iloc[0,:]['AreaShape_Area']\n",
        "x2  = df.iloc[0,:]['AreaShape_BoundingBoxMinimum_Y']\n",
        "x3  = df.iloc[0,:]['AreaShape_Eccentricity']\n",
        "x4  = df.iloc[0,:]['AreaShape_HuMoment_4']\n",
        "x5  = df.iloc[0,:]['AreaShape_NormalizedMoment_1_1']\n",
        "x6  = df.iloc[0,:]['AreaShape_NormalizedMoment_1_2']\n",
        "x7  = df.iloc[0,:]['AreaShape_Zernike_6_2']\n",
        "x8  = df.iloc[0,:]['AreaShape_Zernike_8_6']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaDEXvVcgWLT"
      },
      "outputs": [],
      "source": [
        "def discriminator_loss(real_img, fake_img):\n",
        "    real_loss = tf.reduce_mean(real_img)\n",
        "    fake_loss = tf.reduce_mean(fake_img)\n",
        "    return fake_loss - real_loss\n",
        "\n",
        "\n",
        "# Define the loss functions for the generator.\n",
        "def generator_loss(fake_img):\n",
        "    return -tf.reduce_mean(fake_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxMEPIBH40i9"
      },
      "outputs": [],
      "source": [
        "callback = GANMonitor(num_img=3, latent_dim=latent_dim, feature = [x1,x2,x3,x4,x5,x6,x7,x8])\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJOvUjVvR4XF"
      },
      "outputs": [],
      "source": [
        "cond_gan = ConditionalGAN(\n",
        "    discriminator=get_discriminator(),\n",
        "    generator=get_geneator(),\n",
        "    latent_dim=latent_dim ,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "cond_gan.compile(\n",
        "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.5, beta_2=0.9),\n",
        "    g_optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.5, beta_2=0.9),\n",
        "    g_loss_fn=generator_loss,\n",
        "    d_loss_fn=discriminator_loss,\n",
        ")\n",
        "\n",
        "hist=cond_gan.fit(train_dataset, epochs=3000 , callbacks=[callback,tensorboard_callback])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIDlRvr2HPmZ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7,7))\n",
        "plt.plot(hist.epoch,hist.history['g_loss'])\n",
        "plt.plot(hist.epoch,hist.history['d_loss'])\n",
        "plt.legend(['Generator loss','Discriminator loss'])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}